# 아파치 카프카의  탄생
---
### 카프카의 탄생
- '링크드인'의 파편화된 데이터 수집 및 분배 아키텍처를 운여하는데 어려움을 해결하기 위해 탄생했다
- 아키텍처가 거대해지면서 소스 애플리케이션과 타겟 애플리케이션의 개수가 많아지면서 문제가 발생했다
	- 데이터를 전송하는 라인이 기하급수적으로 복잡해졌다
- 기존에 나와 있던 상용 프레임워크와 오픈소스 아키텍처를 녹여내어 데이터 파이프라인의 파편화를 개선하려 했다
- 다양한 메시징 플랫폼과 ETL(Extract Transform Load) 툴을 이용하여 아키텍처를 변경하려 노력했지만, 아키텍처의 복잡도를 낮춰주지는 못했다

**카프카를 적용한 아키텍처**
![image](https://user-images.githubusercontent.com/60502370/203882441-e20d9394-15fa-4f7d-b3a2-8ad416b28e23.png)
- 카프카는 데이터를 한 곳에 모아 처리할 수 있는 중앙 집중형 아키텍처를 가능하게 했다


### 카프카의 내부 구조
![image](https://user-images.githubusercontent.com/60502370/203882669-8b5895b3-a209-4c3c-bd95-a242de310fc9.png)
- 소스 애플리케이션은 어떤 타겟에 보낼지 고민하지 않고 일단 카프카로 데이터를 전송하면된다
- 토픽은 데이터의 유형을 나타낸다. RDB의 테이블과 유사하다
- 토픽은 내부적으로 한 개 이상의 파티션을 가지고 있다
	- 파티션에 데이터가 저장된다
	- 컨슈머가 데이터를 가져가도 데이터가 삭제되지 않는다
	- 프로듀서가 데이터를 전송하면 한 파티션에만 데이터가 저장된다
	- 파티션의 동작은 큐와 비슷한 FIFO 방식으로 동작한다
- 프로듀서는 데이터를 전송하는 애플리케이션이고 컨슈머는 데이터를 사용하는 애플리케이션이다
- 컨슈머가 어떤 파티션에서 어떤 데이터까지 읽었는지 저장하는데 이를 커밋이라 한다

# 카프카가 데이터 파이프라인으로 적합한 이유
---
## 높은 처리량
- 카프카는 프로듀서가 브로커로 데이터를 보낼 때와 컨슈머가 브로커로부터 데이터를 받을 때 모두 묶어서 전송한다
	- 많은 양의 데이터를 송수신할 때 생기는데 네트워크 비용은 무시할 수 없는 규모가 된다
	- 동일한 양의 데이터를 전송할 때 네트워크 통신을 최소한으로 줄인다면, 동일시간 내 더 많은 데이터를 전송할 수 있다
- 카프카는 많은 양의 데이터를 묶음 단위로 처리하는 배치로 빠르게 처리할 수 있어 대용량 실시간 로그데이터를 처리하는데 적합하다
- 파티션 단위를 통해 동일 목적의 데이터를 여러 파티션에 분배하고 데이터를 병렬로 처리할 수 있다
	- 파티션의 개수만큼 컨슈머 개수를 늘려서 동일 시간당 데이터 처리량을 늘리는 것이다

## 확장성
- 카프카는 가변적인 환경에서 안정적으로 확장 가능하게 설계되었다
- 데이터가 적을 때는 브로커의 개수를 최소한으로 줄이고, 데이터가 많아지면 브로커의 개수를 늘려 스케일 아웃한다
- 다시 데이터가 적어지면 브로커의 개수를 줄여 스케일 인 한다

## 영속성
- 카프카는 데이터를 메모리에 저장하지 않고 파일 시스템에 저장한다
- 운영체제에서는 파일 I/O 성능 향상을 위해 페이지 캐시 영역을 메모리에 따로 생성하여 사용한다
- 카프카는 페이지 캐시 메모리 영역을 사용하여 한 번 읽은 파일 내용을 메모리에 저장했다가 다시 사용하여 처리량을 높였다
- 디스크 기반의 파일 시스템을 사용하기 때문에 애플리케이션에 장애가 발생하여 급작스럽게 종료되더라도, 프로세스를 재시작하여 안전하게 데이터를 다시 처리할 수 있다

## 고가용성
- 3개 이상의 서버들로 구성되는 카프카 클러스터는 일부 서버에 장애가 발생하더라도 무중단으로 안전하고 지속적으로 데이터를 처리할 수 있다
- 클러스터로 이루어진 카프카는 데이터의 복제(Replication)을 통해 고가용성의 특징을 가지게 되었다
	- 프로듀서에게 전달받은 데이터를 다른 브로커에도 저장한다
- 온프레미스 환경의 서버 랙 또는 퍼블릭 클라우드의 리전단위 장애에도 데이터를 안전하게 복제할 수 있는 여러 옵션을 가지고 있다

# 빅데이터 아키텍처의 종류와 카프카의 미래
---
## 초기 빅데이터 플랫폼
![image](https://user-images.githubusercontent.com/60502370/203888840-d58238a3-b9bc-40e6-bf6d-27a5bc57410b.png)
- 초기 빅데이터 플랫폼은 엔드 투 엔드로 각 서비스 애플리케이션으로부터 데이터를 배치로 모았다
- 데이터를 배치로 모으는 구조의 단점
	- 유연하지 못한 구조
	- 실시간으로 생성되는 데이터들에 대한 인사이트를 서비스 애플리케이션에 빠르게 전달하지 못함
	- 원천 데이터로부터 파생된 데이터의 히스토리를 파악하기 어려움
	- 데이터의 가공으로 인해 데이터가 파편화되어 데이터 거버넌스를 지키기 어려움

## 람다 아키텍처
![image](https://user-images.githubusercontent.com/60502370/203889151-4f1e5b77-e5f5-4e6d-98e0-2139b6b70f0c.png)
- 배치 레이어는 배치 데이터를 모아서 특정 시간, 타이밍마다 일괄 처리한다
- 서빙 레이어틑 가공된 데이터를 사용자, 서비스 애플리케이션이 사용할 수 있도록한다
- 스피드 레이어는 서비스에서 생성되는 원천 데이터를 실시간으로 분석한다
- 배치 레이어에 비해 낮은 지연으로 분석이 필요한 경우 스피드 레이어를 통해 데이터를 분석한다

### 람다 아키텍처의 한계
- 데이터를 분석, 처리하는데 필요한 로직이 2벌로 각각 레이어에 따로 존재해야 한다
- 배치 데이터와 실시간 데이터를 융햡하여 처리할 때는 다소 유연하지 못한 파이프 라인을 생성해야 한다

## 카파 아키텍처
![image](https://user-images.githubusercontent.com/60502370/203889582-f5f6370d-db61-4722-a340-d47fc0aa2db9.png)
- 람다 아키텍처의 단점을 보완하기 위한 아키텍처
	- 로직의 파편화
	- 디버깅, 배포, 운영 분리
- 배치 레이어를 제거하였다
- 스피드 레이어에서 데이터를 모두 처리할 수 있어 엔지니어들의 효율적인 개발과 운영을 가능하게 헀다

### 카파 아키텍처 활용
![image](https://user-images.githubusercontent.com/60502370/203889941-0f33b732-896d-4078-b9b2-5c53afabdde6.png)
- 로그는 배치 데이터를 스트림으로 표현하기에 적합하다
- 일반적으로 데이터 플랫폼에서 배치 데이터를 표현할 때는 각 시점의 전체 데이터를 백업한 스냅샷 데이터를 뜻했다
- 하지만 배치 데이터를 로그로 표현할 때는 각 시점의 배치 데이터의 변환 기록을 시간 순서대로 기록하여 각 시점의 모든 스냅샷 데이터를 저장하지 않고도 배치 데이터를 표현할 수 있게 되었다

### 배치 데이터와 스트림 데이터
- 배치 데이터
	- 한정된 데이터 처리
	- 대규모 배치 데이터를 위한 분산 처리 수행
	- 분, 시간, 일 단위 처리를 위한 지연 발생
	- 복잡한 키 조인 수행
- 스트림 데이터
	- 무한 데이터 처리
	- 지속적으로 들어오는 데이터를 위한 분산 처리 수행
	- 분 단위 이하 지연 발생
	- 단순한 키 조인 수행

### 카프카가 스트림데이터를 배치로 사용하는 방법
![Screen Shot 2022-11-25 at 11 51 34 AM](https://user-images.githubusercontent.com/60502370/203890800-5b22f2ef-3dc1-465d-b4ac-81b8a573f4c8.png)
- 스트림 데이터를 배치 데이터로 사용하는 방법은 로그에 시간을 남기는 것
- 로그에 남겨진 시간을 기준으로 데이터를 처리하면 스트림으로 적재된 데이터도 배치로 처리할 수 있게 된다